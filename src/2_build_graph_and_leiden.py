#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
src/2_leiden_fast_final_fixed.py
å­—ç¬¦ä¸²â†’æ•´æ•°èŠ‚ç‚¹ + æœ‰å‘ Leiden + åŠ æƒæŠ•ç¥¨ + PNG/GEXF è¾“å‡º
åƒä¸‡è¾¹ 3-5 min è·‘å®Œï¼ˆå·²ä¿®å¤æ˜ å°„ä¸ç±»å‹é”™è¯¯ï¼‰
"""
import pandas as pd
import igraph as ig
import leidenalg as la
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
import time, os, warnings

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')

# ===================== 1. å‚æ•° =====================
DATA_PATH   = 'D:/Pycharm/Intermediaries_digging/output/cleaned_transactions.csv'
RESOLUTION  = 0.05                       # å•åˆ†è¾¨ç‡ï¼ˆé¢„è°ƒä¼˜ï¼‰
MIN_COMM_SIZE = 10                       # ç¤¾åŒºæœ€å°èŠ‚ç‚¹æ•°
os.makedirs('output', exist_ok=True)

# é»‘æ ·æœ¬åŠ æƒ & é€šé“/æ—¶é—´è¡°å‡
WEIGHT_MAP = {'ç°': 1, 'é»‘': 2, 'é»‘å¯†æ¥': 3, 'é»‘æ¬¡å¯†æ¥': 2, 'é»‘æ¬¡æ¬¡å¯†æ¥': 2, 'é»‘æ¬¡æ¬¡æ¬¡å¯†æ¥': 2, 'æ— å…³': 0, 'æœªçŸ¥': 0}
CHANNEL_W  = {'1': 1.0, '2': 1.0, '4': 1.2, '5': 1.5, '6': 1.0, 'iwl': 1.8, '9': 1.3}
HALF_LIFE  = 90        # å¤©

# ===================== 2. å­—ç¬¦ä¸²â†’æ•´æ•°èŠ‚ç‚¹ + groupby å»ºå›¾ =====================
print('=== 2. å­—ç¬¦ä¸²â†’æ•´æ•°èŠ‚ç‚¹ + groupby å»ºå›¾ ===')
start_t = time.time()

# 1. æ„é€ å­—ç¬¦ä¸²è¾¹è¡¨
df = pd.read_csv(DATA_PATH, usecols=['account_id','counterparty_id','direction','amount','timestamp','channel'])
df['src'] = np.where(df['direction']=='2', df['account_id'], df['counterparty_id'])
df['tgt'] = np.where(df['direction']=='2', df['counterparty_id'], df['account_id'])

# 2. é€šé“æƒé‡ + æ—¶é—´è¡°å‡
df['days']   = (pd.to_datetime('today') - pd.to_datetime(df['timestamp'])).dt.days
df['decay']  = 2 ** (-df['days'] / HALF_LIFE)
df['ch_w']   = df['channel'].map(CHANNEL_W).fillna(1.0)
df['weight'] = df['amount'] * df['decay'] * df['ch_w']

# 3. ç´¯åŠ åŒå‘è¾¹æƒé‡
edge_str = df.groupby(['src','tgt'], as_index=False).agg(weight=('weight','sum'),
                                                         amount=('amount','sum'))

# 4. å­—ç¬¦ä¸²â†’0-based æ•´æ•°
unique_nodes = pd.concat([edge_str['src'], edge_str['tgt']]).unique()
node_map = {n: i for i, n in enumerate(unique_nodes)}
edge_str['src_id'] = edge_str['src'].map(node_map)
edge_str['tgt_id'] = edge_str['tgt'].map(node_map)

# 5. igraph å»ºå›¾ï¼ˆæ•´æ•°ï¼‰
g = ig.Graph.DataFrame(edge_str[['src_id','tgt_id']], directed=True)
g.es['weight'] = edge_str['weight']
g.es['amount'] = edge_str['amount']
g.vs['name']   = unique_nodes          # åŸå­—ç¬¦ä¸² id

print(f'æœ‰å‘å›¾å»ºæˆ |V|={g.vcount():,} |E|={g.ecount():,}  t={time.time()-start_t:.2f}s')

# ===================== 3. å•åˆ†è¾¨ç‡ Leiden + åŠ æƒæŠ•ç¥¨ =====================
print('\n=== 3. å•åˆ†è¾¨ç‡ Leiden + åŠ æƒæŠ•ç¥¨ ===')
start_t = time.time()
partition = la.find_partition(g, la.CPMVertexPartition,
                              weights='weight',
                              resolution_parameter=RESOLUTION,
                              n_iterations=-1)

account_df = pd.read_csv('D:/Pycharm/Intermediaries_digging/output/cleaned_accounts.csv')[['account_id','label']]
label_weight = account_df.drop_duplicates(subset='account_id').set_index('account_id')['label'].map(WEIGHT_MAP).to_dict()

# ç¤¾åŒºåŠ æƒæŠ•ç¥¨
community_df = pd.DataFrame({'account_id': g.vs['name'],
                             'community_id': partition.membership})
community_df['w'] = community_df['account_id'].map(label_weight).fillna(0)
comm_stat = (community_df.groupby('community_id')
                         .agg(total_nodes=('account_id','count'),
                              total_w=('w','sum'))
                         .query('total_nodes >= @MIN_COMM_SIZE')
                         .assign(bad_ratio=lambda d: d['total_w']/d['total_nodes'])
                         .sort_values('bad_ratio', ascending=False))

print('\n Top-10 é«˜é£é™©ç¤¾åŒºï¼ˆåŠ æƒæŠ•ç¥¨ï¼‰')
print(comm_stat.head(10))

# ===================== 4. èŠ‚ç‚¹çº§æ‹“æ‰‘ç‰¹å¾ =====================
print('\n=== 4. èŠ‚ç‚¹çº§æ‹“æ‰‘ç‰¹å¾ ===')
pr  = g.pagerank(weights='weight')
btw = g.betweenness(directed=True, weights='weight')
in_s  = g.strength(mode='in',  weights='weight')
out_s = g.strength(mode='out', weights='weight')

# å»é‡åç”Ÿæˆå”¯ä¸€æ ‡ç­¾æ˜ å°„
label_map = account_df.drop_duplicates(subset='account_id').set_index('account_id')['label']
node_feat = pd.DataFrame({
        'account_id': g.vs['name'],
        'in_strength': in_s,
        'out_strength': out_s,
        'net_flow': np.array(out_s) - np.array(in_s),
        'pagerank': pr,
        'betweenness': btw,
        'community_id': partition.membership,
        'label': community_df['account_id'].map(label_map).fillna('æœªçŸ¥')
})
print(f'ç‰¹å¾å®Œæˆï¼Œshape={node_feat.shape}')

# ===================== 5. ä¿å­˜ç»“æœ =====================
community_df.to_csv('output/leiden_fast_final_fixed_communities.csv', index=False, encoding='utf-8-sig')
comm_stat.to_csv('output/leiden_fast_final_fixed_community_risk.csv', encoding='utf-8-sig')
node_feat.to_csv('output/leiden_fast_final_fixed_node_features.csv', index=False, encoding='utf-8-sig')
print('âœ… å·²ä¿å­˜ï¼šleiden_fast_final_fixed_*.csv')

# ===================== 6. å¯è§†åŒ– & GEXF =====================
top_comm = comm_stat.index[0]
vis_nodes = community_df.query('community_id == @top_comm')['account_id'].tolist()
if len(vis_nodes) > 200:
    vis_nodes = vis_nodes[:200]

sub_g = g.induced_subgraph(vis_nodes)
sub_nx = nx.DiGraph()
sub_nx.add_weighted_edges_from([(sub_g.vs[e.source]['name'],
                                  sub_g.vs[e.target]['name'],
                                  sub_g.es['weight'][e.index]) for e in sub_g.es])

# å…ˆ merge å‡º labelï¼Œå†å»é‡æ˜ å°„
viz_df = (community_df[['account_id']]
          .merge(account_df[['account_id','label']], on='account_id', how='left')
          .fillna({'label':'æœªçŸ¥'})
          .drop_duplicates(subset='account_id')
          .set_index('account_id')['label'])

color_map = ['red' if viz_df.get(n) in ['ç°','é»‘','é»‘å¯†æ¥'] else 'lightblue' for n in sub_nx.nodes()]

plt.figure(figsize=(14, 14))
pos = nx.spring_layout(sub_nx, seed=42, k=1.5, iterations=50)
nx.draw_networkx_nodes(sub_nx, pos, node_color=color_map, node_size=80, alpha=0.9)
nx.draw_networkx_edges(sub_nx, pos, edge_color='grey', alpha=0.4, width=0.5)
plt.title(f'Leiden Fast Final Fixed High-Risk Community {top_comm}  (Q={partition.quality():.3f})')
plt.axis('off')
plt.tight_layout()
plt.savefig('output/leiden_fast_final_fixed_high_risk_comm.png', dpi=150, bbox_inches='tight')
plt.show()

nx.write_gexf(sub_nx, 'output/leiden_fast_final_fixed_high_risk_comm.gexf')
print('\nğŸ‰ Leiden Fast Final Fixed å…¨æµç¨‹å®Œæˆï¼æœ€ä½³åˆ†è¾¨ç‡ã€åŠ æƒ F1ã€GEXF å·²è¾“å‡º')

"""#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
#src/2_build_graph_and_leiden_final.py
#ç»ˆæç”Ÿäº§ç‰ˆï¼šç½‘æ ¼åˆ†è¾¨ç‡ + åŠ æƒæŠ•ç¥¨ + æœ‰å‘ Leiden + é€šé“/æ—¶é—´æƒé‡
"""
import pandas as pd
import igraph as ig
import leidenalg as la
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
import time, os, warnings

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')

# ===================== 1. å‚æ•°åŒº =====================
DATA_PATH   = 'D:/Pycharm/Intermediaries_digging/output/cleaned_transactions.csv'
CHUNK_SIZE  = 1_000_000
MIN_COMM_SIZE = 10                       # è¿‡æ»¤å¾®å‹ç¤¾åŒº
VIS_TOP_K   = 200
os.makedirs('output', exist_ok=True)

# é»‘æ ·æœ¬æƒé‡æ˜ å°„ï¼ˆlabel â†’ æŠ•ç¥¨æƒé‡ï¼‰
WEIGHT_MAP = {'ç°': 1, 'é»‘': 2, 'é»‘å¯†æ¥': 3, 'é»‘æ¬¡å¯†æ¥': 2, 'é»‘æ¬¡æ¬¡å¯†æ¥': 2, 'é»‘æ¬¡æ¬¡æ¬¡å¯†æ¥': 2, 'æ— å…³': 0, 'æœªçŸ¥': 0}

# é€šé“æƒé‡ï¼ˆå¯é€‰ï¼‰
CHANNEL_W = {'1': 1.0, '2': 1.0, '4': 1.2, '5': 1.5, '6': 1.0, 'iwl': 1.8, '9': 1.3}

# æ—¶é—´è¡°å‡åŠè¡°æœŸï¼ˆå¤©ï¼‰
HALF_LIFE = 90

# ===================== 2. åˆ†å—å»ºã€Œæœ‰å‘æƒé‡å›¾ã€ =====================
print('=== 2. åˆ†å—å»ºã€Œæœ‰å‘æƒé‡å›¾ã€ ===')
start_t = time.time()

edge_dict = {}         # (src, tgt) -> cumulative_weight
node_set  = set()
reader = pd.read_csv(DATA_PATH, chunksize=CHUNK_SIZE,
                     usecols=['account_id','counterparty_id','direction','amount','timestamp','channel'])

for k, chunk in enumerate(reader, 1):
    chunk['days'] = (pd.to_datetime('today') - pd.to_datetime(chunk['timestamp'])).dt.days
    chunk['decay'] = 2 ** (-chunk['days'] / HALF_LIFE)          # æŒ‡æ•°è¡°å‡
    chunk['ch_w'] = chunk['channel'].map(CHANNEL_W).fillna(1.0)
    chunk['weight'] = chunk['amount'] * chunk['decay'] * chunk['ch_w']

    for _, r in chunk.iterrows():
        if r.direction == '2':
            src, tgt = r.account_id, r.counterparty_id
        else:
            src, tgt = r.counterparty_id, r.account_id
        key = (src, tgt)
        edge_dict[key] = edge_dict.get(key, 0) + r.weight
        node_set.update([src, tgt])

    print(f'  chunk {k} processed, current edges {len(edge_dict):,}')

# å»º igraph æœ‰å‘å›¾
edges   = list(edge_dict.keys())
weights = list(edge_dict.values())
g = ig.Graph(directed=True)
g.add_vertices(list(node_set))
g.add_edges(edges)
g.es['weight'] = weights

print(f'æœ‰å‘å›¾å»ºæˆ |V|={g.vcount():,} |E|={g.ecount():,}  t={time.time()-start_t:.2f}s')

# ===================== 3. ç½‘æ ¼æœåˆ†è¾¨ç‡ + åŠ æƒæŠ•ç¥¨ =====================
account_df = pd.read_csv('D:/Pycharm/Intermediaries_digging/output/cleaned_accounts.csv')[['account_id','label']]
label_weight = account_df.set_index('account_id')['label'].map(WEIGHT_MAP).to_dict()

res_grid = [0.02, 0.05, 0.08, 0.1, 0.15]
best_f1  = 0
best_res = None
best_part= None

print('\n=== 3. ç½‘æ ¼æœåˆ†è¾¨ç‡ï¼ˆåŠ æƒæŠ•ç¥¨ F1ï¼‰===')
for res in res_grid:
    partition = la.find_partition(g, la.CPMVertexPartition,
                                  weights='weight',
                                  resolution_parameter=res,
                                  n_iterations=-1)
    # ç¤¾åŒºçº§åŠ æƒæŠ•ç¥¨
    comm_df = pd.DataFrame({'account_id': g.vs['name'],
                            'community_id': partition.membership})
    comm_df['w'] = comm_df['account_id'].map(label_weight).fillna(0)
    comm_stat = (comm_df.groupby('community_id')
                        .agg(total_nodes=('account_id','count'),
                             total_w=('w','sum'))
                        .query('total_nodes >= @MIN_COMM_SIZE')
                        .assign(bad_ratio=lambda d: d['total_w']/d['total_nodes'])
                        .sort_values('bad_ratio', ascending=False))

    # è¯„ä¼°ï¼šbad_ratio > 0.2 è§†ä¸ºé«˜å±ç¤¾åŒº
    pred_nodes = comm_df[comm_df['community_id'].isin(
                        comm_stat[comm_stat['bad_ratio']>0.2].index)]['account_id'].unique()
    truth = account_df[account_df['label'].isin(['ç°','é»‘','é»‘å¯†æ¥','é»‘æ¬¡å¯†æ¥','é»‘æ¬¡æ¬¡å¯†æ¥'])]['account_id'].values
    tp = len(set(truth) & set(pred_nodes))
    prec = tp / len(pred_nodes) if pred_nodes.size else 0
    rec  = tp / len(truth) if truth.size else 0
    f1   = 2*prec*rec/(prec+rec) if (prec+rec) else 0
    print(f'  res={res}  F1={f1:.3f}  Q={partition.quality():.3f}')
    if f1 > best_f1:
        best_f1, best_res, best_part = f1, res, partition

print(f'æœ€ä½³åˆ†è¾¨ç‡ = {best_res}  F1 = {best_f1:.3f}')

# ===================== 4. æœ€ç»ˆç¤¾åŒºç»“æœ =====================
community_df = pd.DataFrame({'account_id': g.vs['name'],
                             'community_id': best_part.membership})
community_df['w'] = community_df['account_id'].map(label_weight).fillna(0)
comm_stat_final = (community_df.groupby('community_id')
                                .agg(total_nodes=('account_id','count'),
                                     total_w=('w','sum'))
                                .query('total_nodes >= @MIN_COMM_SIZE')
                                .assign(bad_ratio=lambda d: d['total_w']/d['total_nodes'])
                                .sort_values('bad_ratio', ascending=False))

print('\n Top-10 é«˜é£é™©ç¤¾åŒºï¼ˆåŠ æƒæŠ•ç¥¨ï¼‰')
print(comm_stat_final.head(10))

# ===================== 5. èŠ‚ç‚¹çº§ç‰¹å¾ =====================
print('\n=== 4. èŠ‚ç‚¹çº§æ‹“æ‰‘ç‰¹å¾ ===')
start_t = time.time()
pr  = g.pagerank(weights='weight')
btw = g.betweenness(directed=True, weights='weight')
in_s  = g.strength(mode='in',  weights='weight')
out_s = g.strength(mode='out', weights='weight')

node_feat = pd.DataFrame({
        'account_id': g.vs['name'],
        'in_strength': in_s,
        'out_strength': out_s,
        'net_flow': out_s - in_s,
        'pagerank': pr,
        'betweenness': btw,
        'community_id': best_part.membership,
        'label': community_df['account_id'].map(account_df.set_index('account_id')['label'])
})
print(f'ç‰¹å¾å®Œæˆï¼Œshape={node_feat.shape}  t={time.time()-start_t:.2f}s')

# ===================== 6. ä¿å­˜ç»“æœ =====================
community_df.to_csv('output/leiden_final_communities.csv', index=False, encoding='utf-8-sig')
comm_stat_final.to_csv('output/leiden_final_community_risk.csv', encoding='utf-8-sig')
node_feat.to_csv('output/leiden_final_node_features.csv', index=False, encoding='utf-8-sig')
print(f'å·²ä¿å­˜ï¼šleiden_final_*.csv')

# ===================== 7. å¯è§†åŒ–æœ€å¤§é«˜é£é™©ç¤¾åŒº =====================
top_comm = comm_stat_final.index[0]
vis_nodes = community_df.query('community_id == @top_comm')['account_id'].tolist()
if len(vis_nodes) > 200:
    vis_nodes = vis_nodes[:200]

sub_g = g.induced_subgraph(vis_nodes)
sub_nx = nx.DiGraph()
sub_nx.add_weighted_edges_from([(sub_g.vs[e.source]['name'],
                                  sub_g.vs[e.target]['name'],
                                  sub_g.es['weight'][e.index]) for e in sub_g.es])

label_map = community_df.drop_duplicates('account_id').set_index('account_id')['label']
color_map = ['red' if label_map.get(n) in ['ç°','é»‘','é»‘å¯†æ¥'] else 'lightblue' for n in sub_nx.nodes()]

plt.figure(figsize=(14, 14))
pos = nx.spring_layout(sub_nx, seed=42, k=1.5, iterations=50)
nx.draw_networkx_nodes(sub_nx, pos, node_color=color_map, node_size=80, alpha=0.9)
nx.draw_networkx_edges(sub_nx, pos, edge_color='grey', alpha=0.4, width=0.5)
plt.title(f'Leiden Final High-Risk Community {top_comm}  (res={best_res}, Q={best_part.quality():.3f})')
plt.axis('off')
plt.tight_layout()
plt.savefig('output/leiden_final_high_risk_comm.png', dpi=150, bbox_inches='tight')
plt.show()

# ===================== 8. å¯¼å‡º GEXFï¼ˆGephi/Pyvis ç›´æ¥æ‹–ï¼‰ =====================
nx.write_gexf(sub_nx, 'output/leiden_final_high_risk_comm.gexf')
print('\n Leiden Final å…¨æµç¨‹å®Œæˆï¼æœ€ä½³åˆ†è¾¨ç‡ã€åŠ æƒ F1ã€GEXF å·²è¾“å‡º') """